\documentclass[12pt]{article}

\usepackage{bm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{cite}
\usepackage{indentfirst}
\usepackage{url}
\usepackage{graphicx}
\usepackage{changepage}
\usepackage{pythonhighlight}

\title{Project 4 \\\vspace{1em}
\large CS 525: Deep Learning}
\author{
  Jerry Duncan \\
  jdunca51@vols.utk.edu
}
\date{April 20, 2020}
\begin{document}
\maketitle
\pagebreak

\section{Introduction}

For this project, we're testing different RNNs on their ability to predict the next character based on a small input sequence.
We'll be looking at different types of RNN layers, such as SimpleRNN and LSTM layers, as well as trying different input sequence lengths and strides.
We'll then use these networks to generate some text given an input sequence to see how well they are able to produce text that, at a minimum, consists of real words, and hopefully makes logical sense.

\section{Networks}

For this project, I'll be using two different networks.
The first uses a SimpleRNN layer with a fully connected layer, with a variable number of hidden units.
Similarly, the second uses a LSTM layer with a fully connected layer, with a variable number of hidden units.
I decided to tweak the number of hidden units instead of experimenting with a variable number of layers due to training time and thoroughness of experiments.
\begin{python}
  # SimpleRNN with a single layer attached to a FCL for classification
  def srnn(self, nb_hidden):
  return Sequential([
      SimpleRNN(nb_hidden, input_dim=self.vocab_size),
      Dense(self.vocab_size, activation="softmax"),
    ])
\end{python}
\begin{python}
  # LSTM with a single layer attached to a FCL for classification
  def lstm(self, nb_hidden):
  return Sequential([
      LSTM(nb_hidden, input_dim=self.vocab_size),
      Dense(self.vocab_size, activation="softmax"),
    ])
\end{python}

\section{Results}

For my experiments, I tried two configurations for the hidden unit size, the window length, and the stride.
I did these for both networks and generated graphs to see the loss of each network over 100 epochs.

In Figure \ref{fig:srnn}, we can see the losses of the SimpleRNN experiments.
The first thing to note is that none of them managed to bring the loss below 1.6.
Another thing is that the networks with 100 hidden units performed worse in all cases.
While it's typically known that there is an upper limit on hidden units to get better performance, 100 hidden units is not very many compared to other modern deep neural networks.

In Figure \ref{fig:lstm}, we can see the losses of the LSTM experiments.
This time one configuration managed to bring the loss below 0.5, and all of them were under 1.25.
These results seem to make more sense from the perspective of other modern deep neural networks because more hidden units allows the LSTM networks to perform better.
It also seems that a larger stride affects loss more than window size regardless of the number of hidden units.

\begin{figure}[!htb]
  \centering
  \includegraphics[width=\linewidth]{"model_srnn_hidden_50,100_window_6,12_stride_3,6.png}
  \caption{Loss of SimpleRNN networks with different hyperparameter configurations.}
  \label{fig:srnn}
\end{figure}

\begin{figure}[!htb]
  \centering
  \includegraphics[width=\linewidth]{"model_lstm_hidden_50,100_window_6,12_stride_3,6.png}
  \caption{Loss of LSTM networks with different hyperparameter configurations.}
  \label{fig:lstm}
\end{figure}

The next thing we want to look at to see if the losses actually matter, is how well each network can generate text.
In Figure \ref{tab:text}, we show predicted text given the initial character sequence after every 20 epochs.
The first thing to point out is that no network was able to produce the training text sequence that the initial characters were pulled from.
The second is that the SimpleRNN networks produces more sequences containing characters that weren't real words.
The third is that often the best generated sentences came after 20 and 80 epochs and that many seemed to have some serious problems after 100 epochs.
And lastly, the two SimpleRNN networks that had losses that spiked above 2.5 essentially generated gibberish.

\begin{figure}[!htb]
  \tiny
  \caption{Text generated by different hyperparameter combinations.
    Columns are the two types of RNNs and rows are different hyperparameters.
  }
  \label{tab:text}
  \begin{tabular}{lll}
                               & LSTM                       & SimpleRNN                  \\
    \begin{tabular}[c]{@{}l@{}}50 hidden,\\ 6 window,\\ 3 stride\end{tabular}  & \begin{tabular}[c]{@{}l@{}}Initial chars: ` as fr`\\ Correct text:  as from today, well, i've seen some\\ Predicted text:  as from me and she love to and i sa\\ Predicted text:  as from my friend. i say the way th\\ Predicted text:  as from my friends i’m the only lon\\ Predicted text:  as from my friend i said i’ll go it\\ Predicted text:  as fret rebyb ofoyean the only to t\end{tabular}  & \begin{tabular}[c]{@{}l@{}}Initial chars: `oked s`\\ Correct text: oked so fierce, his mother butted in\\ Predicted text: oked she want to the way, if you say\\ Predicted text: oked she love you a lift a long the\\ Predicted text: oked she want to know i want to know\\ Predicted text: oked so when it and i want somethare\\ Predicted text: oked she want to to the worll now to\end{tabular}  \\
    \begin{tabular}[c]{@{}l@{}}50 hidden,\\ 6 window,\\ 6 stride\end{tabular}  & \begin{tabular}[c]{@{}l@{}}Initial chars: `ee tha`\\ Correct text: ee that he’s just a fool, and he nev\\ Predicted text: ee that i want to do, so how you say\\ Predicted text: ee that i wanta me a now i’m the tim\\ Predicted text: ee that she does, she said i’m on th\\ Predicted text: ee that’s right i can do so ho to ca\\ Predicted text: ee that you know i wore you and i wa\end{tabular}  & \begin{tabular}[c]{@{}l@{}}Initial chars: `i got `\\ Correct text: i got a whole lot of things to tell\\ Predicted text: i got ond on your right i want to kn\\ Predicted text: i got mead the sing the sing the sin\\ Predicted text: i got that you alone you make you ma\\ Predicted text: i got to know you know you know you\\ Predicted text: i got a when they show and that and\end{tabular} \\
    \begin{tabular}[c]{@{}l@{}}50 hidden,\\ 12 window,\\ 3 stride\end{tabular} & \begin{tabular}[c]{@{}l@{}}Initial chars: `it's like to`\\ Correct text: it's like to listen to your fears ch\\ Predicted text: it's like to see you want to like yo\\ Predicted text: it's like to hold you know that she\\ Predicted text: it's like to you she wanted it straw\\ Predicted text: it's like to let you know you know i\\ Predicted text: it's like to know the world it a lit\end{tabular} & \begin{tabular}[c]{@{}l@{}}Initial chars: `t down below`\\ Correct text: t down below his knee hold you in h\\ Predicted text: t down below sean sad she sad the s\\ Predicted text: t down belown the word wain a little\\ Predicted text: t down below she don't got the way i\\ Predicted text: t down below on the cry can in the c\\ Predicted text: t down belowe she she’s need you kno\end{tabular} \\
    \begin{tabular}[c]{@{}l@{}}50 hidden,\\ 12 window,\\ 6 stride\end{tabular} & \begin{tabular}[c]{@{}l@{}}Initial chars: `eal love...`\\ Correct text: eal love... revolution you say you w\\ Predicted text: eal love... but i don't know why you\\ Predicted text: eal love... say her me the cry the m\\ Predicted text: eal love... but no one you say the b\\ Predicted text: eal love... word no know what i want\\ Predicted text: eal love... doesn with me with a lov\end{tabular} & \begin{tabular}[c]{@{}l@{}}Initial chars:  `waiting her`\\ Correct text:  waiting here for you, wond’ring wha\\ Predicted text:  waiting here houn hore, herely hor\\ Predicted text:  waiting her saive when a want to k\\ Predicted text:  waiting here the that’s when you th\\ Predicted text:  waiting her love to come on out lik\\ Predicted text:  waiting here here i want i know i k\end{tabular} \\
    \begin{tabular}[c]{@{}l@{}}100 hidden,\\ 6 window,\\ 3 stride\end{tabular} & \begin{tabular}[c]{@{}l@{}}Initial chars: ` of lo`\\ Correct text:  of love got a hold on me. please be\\ Predicted text:  of love you know that she does, she\\ Predicted text:  of love you know this like you know\\ Predicted text:  of love, love you know i know i kno\\ Predicted text:  of love, love with a geamong a dris\\ Predicted text:  of love, love you. i can't be stay\end{tabular} & \begin{tabular}[c]{@{}l@{}}Initial chars: `an, 'c`\\ Correct text: an, 'cause when i get you alone you\\ Predicted text: an, 'crong the like the like the lik\\ Predicted text: an, 'could i when i won’s she love i\\ Predicted text: an, 'can he sun and i don't the want\\ Predicted text: an, 'could i like it i look it it’s\\ Predicted text: an, 'caus to my love baby bar the wa\end{tabular} \\
    \begin{tabular}[c]{@{}l@{}}100 hidden,\\ 6 window,\\ 6 stride\end{tabular} & \begin{tabular}[c]{@{}l@{}}Initial chars: `oom on`\\ Correct text: oom only to find gideon’s bible gide\\ Predicted text: oom onlow oh, that she magade i will\\ Predicted text: oom only heart be man, yea-midhor bl\\ Predicted text: oom onle hady triam on the sky is i'\\ Predicted text: oom onl maney think tw ywain, and th\\ Predicted text: oom onle wanty you say good day suns\end{tabular} & \begin{tabular}[c]{@{}l@{}}Initial chars: ` a swe`\\ Correct text:  a sweater by the fireside sunday mo\\ Predicted text:  a sweend a sall me when i don’t you\\ Predicted text:  a swees yearle you and are you and\\ Predicted text:  a swerd somes on won’s oh, so sevin\\ Predicted text:  a swee how holl he, i do make help\\ Predicted text:  a swes it and i ne do, it it and i\end{tabular} \\
    \begin{tabular}[c]{@{}l@{}}100 hidden,\\ 12 window,\\ 3 stride\end{tabular} & \begin{tabular}[c]{@{}l@{}}Initial chars: `llow submari`\\ Correct text: llow submarine we all live in our ye\\ Predicted text: llow submaring and way to make you t\\ Predicted text: llow submaring all the love there i\\ Predicted text: llow submaring what i do i’ve know t\\ Predicted text: llow submaring what can i do, and i \\ Predicted text: llow submaring we and knew of the li\end{tabular} & \begin{tabular}[c]{@{}l@{}}Initial chars:  `car yes i'm`\\ Correct text:  car yes i'm gonna be a star baby yo\\ Predicted text:  car yes i'm men't the me the seenee\\ Predicted text:  car yes i'm on tou tou tou bat ond \\ Predicted text:  car yes i'me mane took tome tome to\\ Predicted text:  car yes i'm she she she she she she\\ Predicted text:  car yes i'm that that that that tha\end{tabular} \\
    \begin{tabular}[c]{@{}l@{}}100 hidden,\\ 12 window,\\ 6 stride\end{tabular} & \begin{tabular}[c]{@{}l@{}}Initial chars: ` out. what g`\\ Correct text:  out. what goes on what goes on in y\\ Predicted text:  out. what go. be, i can. you wand \\ Predicted text:  out. what going to know a have her \\ Predicted text:  out. what good a love to es. i can \\ Predicted text:  out. what going home. the be to sel\\ Predicted text:  out. what goob, wind and i should w\end{tabular} & \begin{tabular}[c]{@{}l@{}}Initial chars: `old me you d`\\ Correct text: old me you didn’t need me anymore w\\ Predicted text: old me you das it it i wist i was i\\ Predicted text: old me you dove i kane th aive t at \\ Predicted text: old me you d lantetthettitet tutttel\\ Predicted text: old me you d gongi w ging honging w\\ Predicted text: old me you de i in im win i i in th\end{tabular}
  \end{tabular}
\end{figure}


\section{Conclusion}

In conclusion, LSTM networks perform better than SimpleRNN ones.
LSTM networks also seem to better generalize to more hidden units and can make good use of them as they often have a lower loss the more they have.
In all cases, a larger window size worked better than a smaller one, regardless of stride or number of hidden units.
Overall, the LSTM network with 100 hidden units, a window size of 12, and a stride of 6 worked the best of all networks tested.

\section{Running My Code}

My code is able to be ran like asked for in the project requirements.
\begin{python}
  python rnn.py <filename> <model_name> <nb_hidden> \
  <window_size> <stride>
\end{python}

It can also be run with combinations of parameters using main.py instead.
\end{document}